{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook demonstrates the initial steps of understanding a time series dataset, exploration and visualization.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The [Iowa Liquor Sales](https://console.cloud.google.com/marketplace/details/iowa-department-of-commerce/iowa-liquor-sales) dataset from BigQuery Public Datasets is used in this example. The dataset contains wholesale liquor purchases in the state of Iowa from 2012 to the present.\n",
    "\n",
    "### Objective\n",
    "\n",
    "We will show how to use BigQuery to query data and then use the `statsmodels` stats and `seaborn` visualization packages to explore the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install packages and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarting the kernel may be required to use new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "%pip install -U statsmodels --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To restart the Kernel, navigate to Kernel > Restart Kernel... on the Jupyter menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery as bq\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# Enter your project and region. Then run the  cell to make sure the\n",
    "# Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "PROJECT = \"your-gcp-project-here\" # REPLACE WITH YOUR PROJECT NAME \n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "#Don't change the following command - this is to check if you have changed the project name above.\n",
    "assert PROJECT != 'your-gcp-project-here', 'Don''t forget to change the project variables!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'y'  # What we are predicting\n",
    "ts_col = 'ds'     # Time series column\n",
    "holiday_col = 'holiday'\n",
    "\n",
    "daily_file = 'iowa_daily.csv'\n",
    "monthly_file = 'iowa_monthly.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first sample the dataset\n",
    "\n",
    "sql = 'SELECT * FROM `bigquery-public-data.iowa_liquor_sales.sales` LIMIT 5'\n",
    "\n",
    "client = bq.Client(project=PROJECT)\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all sales by month.\n",
    "# Category names are very specific (e.g. \"Straight Rye Whiskies\"), so let's group them.\n",
    "# The date_trunc() function will extract just the month and day parts for grouping\n",
    "\n",
    "sql_monthly = \"\"\"\n",
    "select\n",
    "  case \n",
    "    when lower(category_name) like '%vodka%' then 'vodka'\n",
    "    when lower(category_name) like '%liqueur%' then 'liqueur'\n",
    "    when lower(category_name) like '%bourbon%' then 'bourbon'\n",
    "    when lower(category_name) like '%scotch%' then 'scotch'    \n",
    "    when lower(category_name) like '%whisk%' then 'whisky'\n",
    "    when lower(category_name) like '%rum%' then 'rum'\n",
    "    when lower(category_name) like '%tequila%' then 'tequila'\n",
    "    when lower(category_name) like '%brand%' then 'brandy'\n",
    "    when lower(category_name) like '%schnapps%' then 'schnapps'\n",
    "    when lower(category_name) like '%gin%' then 'gin'\n",
    "    else 'other'\n",
    "  end as category,\n",
    "  sum(sale_dollars) as y,\n",
    "  date_trunc(date, month) as ds  \n",
    "from `bigquery-public-data.iowa_liquor_sales.sales`\n",
    "GROUP by category, ds ORDER BY ds asc, category asc\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "df_monthly_by_category = client.query(sql_monthly).to_dataframe()\n",
    "\n",
    "# Print the first few rows to see what is returned\n",
    "df_monthly_by_category.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the popularity by category\n",
    "\n",
    "df_category = df_monthly_by_category.groupby('category').sum().sort_values(by=target_col, ascending=False)\n",
    "df_category.head()\n",
    "_ = sns.barplot(x=df_category[target_col], y=df_category.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 1: Analyze the patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the trends for a few categories.\n",
    "\n",
    "# TODO-1: What patterns do you notice? Are there different trajectories? Are there differences in seasonality?\n",
    "\n",
    "sample_categories = ['whisky','vodka','rum']\n",
    "\n",
    "register_matplotlib_converters() # Addresses a warning\n",
    "\n",
    "with plt.rc_context():\n",
    "    plt.rc('figure', figsize=(20,6))\n",
    "    df_monthly_sample_categories = df_monthly_by_category[df_monthly_by_category.category.isin(sample_categories)]\n",
    "    _ = sns.lineplot(x=ts_col, y=target_col, hue='category', data=df_monthly_sample_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now group the data into sales by day (aggregating category-level data)\n",
    "df_monthly = df_monthly_by_category.groupby(ts_col).sum()\n",
    "\n",
    "df_monthly.index = pd.DatetimeIndex(df_monthly.index) # Set index explicitly to a datetime index for future graphing\n",
    "\n",
    "df_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide summary statistics\n",
    "\n",
    "with pd.option_context('display.float_format', '{:,.0f}'.format):\n",
    "    print(df_monthly[target_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time-series data\n",
    "\n",
    "_ = sns.lineplot(data=df_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of values for each month in a boxplot:\n",
    "# Min, 25th percentile, median, 75th percentile, max \n",
    "\n",
    "months = df_monthly.index.to_series().dt.month\n",
    "\n",
    "_ = sns.boxplot(x=months, y=df_monthly[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the data into trend and seasonal components\n",
    "\n",
    "result = seasonal_decompose(df_monthly, period=12)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data for use in a future lab.\n",
    "# This will generate a csv file, which you will use in the next labs of this quest.\n",
    "# Inspect the csv file to see what the data looks like.\n",
    "\n",
    "df_monthly.to_csv(monthly_file, index=True, index_label='ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Daily Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all sales transactions by day\n",
    "\n",
    "sql_daily = \"\"\"\n",
    "SELECT SUM(sale_dollars) as y, date as ds FROM `bigquery-public-data.iowa_liquor_sales.sales`\n",
    "group by ds\n",
    "order by ds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query\n",
    "\n",
    "df_daily = client.query(sql_daily).to_dataframe()\n",
    "\n",
    "df_daily.head()\n",
    "\n",
    "# Are all days provided in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing days in the index\n",
    "\n",
    "df_daily.index = pd.DatetimeIndex(df_daily.pop(ts_col))\n",
    "index_with_missing_vals = pd.date_range(start=min(df_daily.index), end=max(df_daily.index))\n",
    "df_daily = df_daily.reindex(index_with_missing_vals)\n",
    "df_daily.index.freq='D'\n",
    "\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need to replace NaNs with 0\n",
    "\n",
    "df_daily = df_daily.fillna(0)\n",
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide summary statistics\n",
    "\n",
    "with pd.option_context('display.float_format', '{:,.0f}'.format):\n",
    "    print(df_daily[target_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the data. Note the outlier in 2013.\n",
    "\n",
    "result = sns.lineplot(data=df_daily)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 2: Create another line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a subset of the data, to see if there's any weekly pattern.\n",
    "\n",
    "# TODO-2: Create another line plot using only the first 60 days of data\n",
    "# Hint: to get the first n rows of data you can use df_daily[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overall data distribution\n",
    "\n",
    "result = sns.distplot(df_daily)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 3: Create another box plot as you did with monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution by day-of-week\n",
    "\n",
    "# TODO-3: Create another box plot as you did with monthly data, but looking at the distribution by day of week.\n",
    "# Hint: you can use the dt.dayofweek() to get the day of week (0-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See any meaningful difference on holidays?\n",
    "\n",
    "cal = calendar()\n",
    "dr = pd.date_range(start=df_daily.index.to_series().iloc[0], end=df_daily.index.to_series().iloc[-1])\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "df_daily[holiday_col] = df_daily.index.isin(holidays) * 1\n",
    "\n",
    "result = sns.boxplot(x=holiday_col, y=target_col, data=df_daily)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for weekly seasonality (just showing 90 days so pattern is clearer)\n",
    "\n",
    "result = seasonal_decompose(df_daily[target_col][0:90], period=7)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO 4: Try another seasonal decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There can be multiple layers of seasonality - now decomposing by year\n",
    "\n",
    "# TODO-4: Try another seasonal decomposition, but this time apply it to all data (remove the [0:90] slice), and set the period to 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data for use in a future lab.\n",
    "# This will generate a csv file, which you will use in the next labs of this quest.\n",
    "# Inspect the csv file to see what the data looks like.\n",
    "\n",
    "df_daily[holiday_col] = df_daily[holiday_col].astype(float) # Avoids warnings in future labs\n",
    "df_daily.to_csv(daily_file, index=True, index_label='ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've successfully completed the exploration and visualization lab.\n",
    "We've learned how to:\n",
    "* Create a query that groups data into a time series\n",
    "* Fill missing values\n",
    "* Visualize data\n",
    "* Decompose time series into trend and seasonal components"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ai_platform_notebooks_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
