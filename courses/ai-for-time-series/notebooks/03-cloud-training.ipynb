{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook we'll use a [liquor sales dataset](https://www.kaggle.com/residentmario/iowa-liquor-sales) from Kaggle to learn how to submit a job to AI Platform Training. In the job we'll train our TensorFlow 2 model and export the saved model to Cloud Storage.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The [Iowa Liquor Sales](https://console.cloud.google.com/marketplace/details/iowa-department-of-commerce/iowa-liquor-sales) dataset from BigQuery Public Datasets is used in this example. The dataset contains wholesale liquor purchases in the state of Iowa from 2012 to the present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Install packages and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "kRv5imUnKkuP",
    "outputId": "f2f8b527-1b8b-4d7b-c69d-dc9426f62cf6"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import storage\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the TensorFlow version installed\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# Enter your project, region, and bucket. Then run the  cell to make sure the\n",
    "# Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "PROJECT = \"your-project-name\" # REPLACE WITH YOUR PROJECT NAME\n",
    "BUCKET = \"your-bucket-id\" # REPLACE WITH YOUR BUCKET ID\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "BUCKET_URI = \"gs://\" + BUCKET\n",
    "\n",
    "#Don't change the following command - this is to check if you have changed the project name above.\n",
    "assert PROJECT != 'your-project-name', 'Don''t forget to change the project variables!'\n",
    "\n",
    "%env BUCKET_URI=$BUCKET_URI\n",
    "%env REGION=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'y' # What we are predicting\n",
    "ts_col = 'ds' # Time series column\n",
    "if os.path.exists('iowa_daily.csv'):\n",
    "    input_file = 'iowa_daily.csv' # File created in previous lab\n",
    "else:\n",
    "    input_file = 'data/iowa_daily.csv'\n",
    "\n",
    "n_features = 2 # Two features: y (previous values) and whether the date is a holiday\n",
    "n_input_steps = 30 # Lookback window\n",
    "n_output_steps = 1 # How many steps to predict forward\n",
    "\n",
    "train_split = 0.75 # % Split between train/test data\n",
    "epochs = 1000 # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = 5 # Terminate training after the validation loss does not decrease after this many epochs\n",
    "\n",
    "lstm_units = 64\n",
    "\n",
    "model_name = 'retail_forecasting'\n",
    "framework='TENSORFLOW'\n",
    "runtime_version = '2.1'\n",
    "python_version = '3.7'\n",
    "predictions_file = 'predictions.json'\n",
    "input_layer_name = 'lstm_input'\n",
    "\n",
    "%env MODEL_NAME = $model_name\n",
    "%env FRAMEWORK = $framework\n",
    "%env RUNTIME_VERSION = $runtime_version\n",
    "%env PYTHON_VERSION = $python_version\n",
    "%env PREDICTIONS_FILE = $predictions_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. AI Platform runs\n",
    "the code from this package. In this tutorial, AI Platform also saves the\n",
    "trained model that results from your job in the same bucket. You can then\n",
    "create an AI Platform model version based on this output in order to serve\n",
    "online predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET)\n",
    "    print('Bucket exists, let''s not recreate it.')\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET)\n",
    "    print('Created bucket: ' + BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8_SY9abGxCc"
   },
   "source": [
    "## Download and preview the data\n",
    "\n",
    "We've done some pre-processing on the original dataset and made the pre-processed one available on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOfBsktiGCOp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_file, index_col=ts_col, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "DztTBmJEU4bo",
    "outputId": "f9b0154d-a1c6-4b2d-b150-abf64fa61963"
   },
   "outputs": [],
   "source": [
    "# Plot one 30-day window of sales \n",
    "df[target_col][:30].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "067UQKwlVBUf"
   },
   "source": [
    "### Process data and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "59PwFlYDU13-",
    "outputId": "b4b59f59-6d39-4235-8f9f-8a73d7e1cd0e"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "size = int(len(df) * train_split)\n",
    "df_train, df_test = df[0:size].copy(deep=True), df[size:len(df)].copy(deep=True)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "Yn9RSS3DVEtt",
    "outputId": "cd066456-b419-4e0f-8c52-2fd7d82123ba"
   },
   "outputs": [],
   "source": [
    "df_train.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlgFdwM9VOnd"
   },
   "source": [
    "### Scale sales values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review original values\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqRM0Wt6VKzm"
   },
   "outputs": [],
   "source": [
    "# For neural networks to converge quicker, it is helpful to scale the values.\n",
    "# For example, each feature might be transformed to have a mean of 0 and std. dev. of 1.\n",
    "#\n",
    "# We are working with a mix of features, input timesteps, output horizon, etc.\n",
    "# which don't work out-of-the-box with common scaling utilities.\n",
    "# So, here are a couple wrappers to handle scaling and inverting the scaling.\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "def scale(df, \n",
    "          fit=True, \n",
    "          target_col=target_col,\n",
    "          feature_scaler=feature_scaler,\n",
    "          target_scaler=target_scaler):\n",
    "    \"\"\"\n",
    "    Scale the input features, using a separate scaler for the target.\n",
    "    \n",
    "    Parameters: \n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    fit (bool): Whether to fit the scaler to the data (only apply to training data)\n",
    "    target_col (pd.Series): The column that is being predicted\n",
    "    feature_scaler (StandardScaler): Scaler used for features\n",
    "    target_scaler (StandardScaler): Scaler used for target\n",
    "      \n",
    "    Returns: \n",
    "    df_scaled (pd.DataFrame): Scaled dataframe   \n",
    "    \"\"\"    \n",
    "    target = df[target_col].values.reshape(-1, 1)\n",
    "    if fit:\n",
    "        target_scaler.fit(target)\n",
    "    data_scaled = target_scaler.transform(target)\n",
    "    \n",
    "    features = df.loc[:, df.columns != target_col].values\n",
    "    if features.shape[1]:\n",
    "        if fit:\n",
    "            feature_scaler.fit(features)\n",
    "        features_scaled = feature_scaler.transform(features)\n",
    "        data_scaled = np.concatenate([data_scaled, features_scaled], axis=1)\n",
    "\n",
    "    df_scaled = pd.DataFrame(data_scaled, columns=df.columns)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def inverse_scale(data, target_scaler=target_scaler):\n",
    "    \"\"\"\n",
    "    Transform the scaled values of the target back into their original form.\n",
    "    The features are left alone, as we're assuming that the output of the model only includes the target.\n",
    "    \n",
    "    Parameters: \n",
    "    data (np.array): Input array\n",
    "    target_scaler (StandardScaler): Scaler used for target\n",
    "      \n",
    "    Returns: \n",
    "    data_scaled (np.array): Scaled array   \n",
    "    \"\"\"    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    data_scaled = np.empty([data.shape[1], data.shape[0]])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_scaled[i] = target_scaler.inverse_transform(data[:,i])\n",
    "    return data_scaled.transpose()\n",
    "\n",
    "df_train_scaled=scale(df_train)\n",
    "df_test_scaled=scale(df_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "4w2K9hVHXV6-",
    "outputId": "b48986d7-e9b4-4a63-c71c-54b43b1ea7ad"
   },
   "outputs": [],
   "source": [
    "# Review scaled values\n",
    "\n",
    "df_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNw7mVozbsA2"
   },
   "source": [
    "### Create sequences of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76Wp5gRtbrA9"
   },
   "outputs": [],
   "source": [
    "def reframe(data, n_input_steps = n_input_steps, n_output_steps = n_output_steps):\n",
    "\n",
    "    # Iterate through data and create sequences of features and outputs\n",
    "    df = pd.DataFrame(data)\n",
    "    cols=list()\n",
    "    for i in range(n_input_steps, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    for i in range(0, n_output_steps):\n",
    "        cols.append(df.shift(-i))\n",
    "        \n",
    "    # Concatenate values and remove any missing values\n",
    "    df = pd.concat(cols, axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Split the data into feature and target variables\n",
    "    n_feature_cols = n_input_steps * n_features\n",
    "    features = df.iloc[:,0:n_feature_cols]\n",
    "    target_cols = [i for i in range(n_feature_cols, n_feature_cols + n_output_steps * n_features, n_features)]\n",
    "    targets = df.iloc[:,target_cols]\n",
    "\n",
    "    return (features, targets)\n",
    "\n",
    "X_train_reframed, y_train_reframed = reframe(df_train_scaled)\n",
    "X_test_reframed, y_test_reframed = reframe(df_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that X + y values are aligned\n",
    "\n",
    "`df_train_scaled.iloc[n_input_steps + index]['y']` should be equal to `y_train_reframed[index]`, and the same for the test dataset.\n",
    "\n",
    "In other words, the ground truth labels should contain the sales for the next day after the training window (or whatever future projection you're making)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(df_train_scaled.iloc[n_input_steps + i][target_col])\n",
    "    print(y_train_reframed.iloc[i].values[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the same is true for the test set\n",
    "\n",
    "for i in range(3):\n",
    "    print(df_test_scaled.iloc[n_input_steps + i][target_col])\n",
    "    print(y_test_reframed.iloc[i].values[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqhwqDsxb-E_"
   },
   "source": [
    "## Build a model and submit your training job to AI Platform\n",
    "\n",
    "The model we're building here trains pretty fast so we could train it in this notebook, but for more computationally expensive models, it's useful to train them in the Cloud. To show you how AI Platform Training works, we'll package up our training code and submit a training job to the AI Platform Prediction service.\n",
    "\n",
    "In our training script, we'll also export our trained `SavedModel` to a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HG1bYOO8b4sN"
   },
   "outputs": [],
   "source": [
    "# Reshape test data to match model inputs and outputs\n",
    "\n",
    "X_train = X_train_reframed.values.reshape(X_train_reframed.shape[0], n_input_steps, n_features)\n",
    "X_test = X_test_reframed.values.reshape(X_test_reframed.shape[0], n_input_steps, n_features)\n",
    "y_train = y_train_reframed.values.reshape(y_train_reframed.shape[0], n_output_steps)\n",
    "y_test = y_test_reframed.values.reshape(y_test_reframed.shape[0], n_output_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_dir = 'trainer'\n",
    "export_dir = 'tf_export'\n",
    "\n",
    "%env trainer_dir = trainer_dir\n",
    "%env export_dir = export_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $trainer_dir\n",
    "!touch $trainer_dir/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy numpy arrays to npy files\n",
    "\n",
    "np.save(trainer_dir + '/x_train.npy', X_train)\n",
    "np.save(trainer_dir + '/x_test.npy', X_test)\n",
    "np.save(trainer_dir + '/y_train.npy', y_train)\n",
    "np.save(trainer_dir + '/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training code out to a file that will be submitted to the training job\n",
    "# Note: f-strings are supported in Python 3.6 and above\n",
    "\n",
    "model_template = f\"\"\"import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from google.cloud import storage\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "n_features = {n_features} # Two features: y (previous values) and whether the date is a holiday\n",
    "n_input_steps = {n_input_steps} # Lookback window\n",
    "n_output_steps = {n_output_steps} # How many steps to predict forward\n",
    "\n",
    "epochs = {epochs} # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = {patience} # Terminate training after the validation loss does not decrease after this many epochs\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        default=None,\n",
    "        help='URL to store the job output')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    print('args: ', args)\n",
    "    model_dir = args.job_dir\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = model_dir.split('/')[2]\n",
    "    \n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # Get the training data and convert back to np arrays\n",
    "    local_data_dir = os.path.join(os.getcwd(), tempfile.gettempdir())\n",
    "    \n",
    "    data_files = ['x_train.npy', 'y_train.npy', 'x_test.npy', 'y_test.npy']\n",
    " \n",
    "    for i in data_files:\n",
    "        blob = storage.Blob('{trainer_dir}/' + i, bucket)\n",
    "        destination_file = local_data_dir + '/' + i\n",
    "        open(destination_file, 'a').close()\n",
    "        blob.download_to_filename(destination_file)\n",
    "\n",
    "    X_train = np.load(local_data_dir + '/x_train.npy')\n",
    "    y_train = np.load(local_data_dir + '/y_train.npy')\n",
    "    X_test = np.load(local_data_dir + '/x_test.npy')\n",
    "    y_test = np.load(local_data_dir + '/y_test.npy')\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = Sequential([\n",
    "        LSTM({lstm_units}, input_shape=[n_input_steps, n_features], recurrent_activation=None),\n",
    "        Dense(n_output_steps)])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    _ = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[early_stopping])\n",
    "    \n",
    "    # Export the model\n",
    "    export_path = os.path.join(model_dir, '{export_dir}')\n",
    "    model.save(export_path)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(trainer_dir, 'model.py'), 'w') as f:\n",
    "    f.write(model_template.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the train data files to a GCS bucket\n",
    "\n",
    "!gsutil -m cp -r trainer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $BUCKET_URI/$trainer_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this if you need to create a new training job\n",
    "\n",
    "timestamp = str(datetime.datetime.now().time())\n",
    "JOB_NAME = 'caip_training_' + str(int(time.time()))\n",
    "%env JOB_NAME=$JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULE_NAME = trainer_dir + '.model'\n",
    "TRAIN_DIR = os.getcwd() + '/' + trainer_dir\n",
    "JOB_DIR = BUCKET_URI\n",
    "%env TRAIN_DIR=$TRAIN_DIR\n",
    "%env MODULE_NAME=$MODULE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the training job\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --scale-tier basic \\\n",
    "        --package-path $TRAIN_DIR \\\n",
    "        --module-name $MODULE_NAME \\\n",
    "        --job-dir $BUCKET_URI \\\n",
    "        --region $REGION \\\n",
    "        --runtime-version $RUNTIME_VERSION \\\n",
    "        --python-version $PYTHON_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the job status\n",
    "\n",
    "!gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor output of your training job\n",
    "\n",
    "Follow the instructions in the output of the gcloud command above to view the logs from your training job. You can also navigate to the [Jobs Section](https://console.cloud.google.com/ai-platform/jobs) of your Cloud Console to view logs.\n",
    "\n",
    "Once your training job completes successfully, it'll export your trained model as a TensorFlow `SavedModel` and write the output to a directory in your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "ECQHhHfUcFoV",
    "outputId": "adefd734-74c3-4041-f39b-fc0c413a7dd7"
   },
   "outputs": [],
   "source": [
    "# Verify model was exported correctly\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(BUCKET)\n",
    "bucket_files = list(bucket.list_blobs(prefix=export_dir + '/'))\n",
    "\n",
    "# If you see a saved_model.pb and a variables/ and assets/ directory here, it means your model was exported correctly in your training job. Yay!\n",
    "\n",
    "for file in bucket_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List models in project\n",
    "models=$(gcloud ai-platform models list)\n",
    "\n",
    "# Search for model in list; create if not found\n",
    "if !(echo $models | grep -q $MODEL_NAME); then\n",
    "    gcloud ai-platform models create $MODEL_NAME --regions $REGION\n",
    "else\n",
    "    echo 'Model already exists. Skipping create step.'\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = BUCKET_URI +'/' + export_dir\n",
    "version = 'version_' + str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model version\n",
    "\n",
    "!gcloud ai-platform versions create $version \\\n",
    "  --model $model_name \\\n",
    "  --origin $export_path \\\n",
    "  --runtime-version=$runtime_version \\\n",
    "  --framework $framework \\\n",
    "  --python-version=$python_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFglcq4kcd4R"
   },
   "source": [
    "## Get predictions on deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing prediction file\n",
    "\n",
    "if os.path.exists(predictions_file):\n",
    "    !rm $PREDICTIONS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_json = {input_layer_name: X_test[0].tolist()}\n",
    "\n",
    "with open(predictions_file, 'a') as outfile:\n",
    "    json.dump(prediction_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = !gcloud ai-platform predict --model $MODEL_NAME --json-instances=$PREDICTIONS_FILE --format=\"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "\n",
    "preds.pop(0) # Remove warning\n",
    "preds = \"\\n\".join(preds) # Concatenate list of strings into one string\n",
    "preds = json.loads(preds) # Convert JSON string into Python dict\n",
    "\n",
    "pred_val = preds['predictions'][0]['dense'][0] # Access prediction\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print prediction and compare to actual value\n",
    "\n",
    "print('Predicted sales:', int(round(inverse_scale(np.array([[pred_val]]))[0][0])))\n",
    "print('Actual sales:   ', int(round(inverse_scale(np.array([y_test[0]]))[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you've learned how to:\n",
    "* Prepare data and models for training in the cloud\n",
    "* Train your model and monitor the progress of the job with AI Platform Training\n",
    "* Predict using the model with AI Platform Predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "liquor-sales-xai.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
